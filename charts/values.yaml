# Deployment method configuration
deployment:
  method: standard                  # Deployment method: "standard" (K8s resources) or "toolhive" (MCPServer CRD)

# Application image configuration
image:
  repository: ghcr.io/vfarcic/dot-ai  # Container image repository
  tag: "0.165.0"                     # Container image tag - set by CI pipeline during release

# Resource configuration
resources:
  requests:
    memory: "512Mi"  # Minimum memory required
    cpu: "200m"      # Minimum CPU required
  limits:
    memory: "2Gi"    # Maximum memory allowed
    cpu: "1000m"     # Maximum CPU allowed

# Secrets configuration
secrets:
  name: dot-ai-secrets              # Name of the Kubernetes Secret resource
  auth:
    keyName: auth-token             # Key name within the secret for Bearer token auth
    token: ""                       # Auth token value (only needed if chart should create the secret)
  anthropic:
    keyName: anthropic-api-key      # Key name within the secret
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  openai:
    keyName: openai-api-key         # Key name within the secret
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  google:
    keyName: google-api-key         # Key name within the secret
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  xai:
    keyName: xai-api-key            # Key name within the secret
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  moonshot:
    keyName: moonshot-api-key       # Key name within the secret (PRD #237: Kimi K2)
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  customLlm:
    keyName: custom-llm-api-key     # Key name within the secret for custom LLM endpoint
    apiKey: ""                      # API key value (only needed if chart should create the secret)
  customEmbeddings:
    keyName: custom-embeddings-api-key  # Key name within the secret for custom embeddings endpoint
    apiKey: ""                      # API key value (only needed if chart should create the secret)

# ServiceAccount configuration
serviceAccount:
  create: true                      # Create a ServiceAccount
  name: ""                          # ServiceAccount name override (generated if empty)

# Ingress configuration
ingress:
  enabled: false                    # Create Ingress resource
  className: nginx                  # Ingress class name
  host: dot-ai.127.0.0.1.nip.io     # Ingress hostname
  # Annotations required for HTTP transport with SSE (Server-Sent Events)
  # If using different className, update annotations for your ingress controller:
  # - Traefik: traefik.ingress.kubernetes.io/service.sticky.cookie.httponly: "true"
  # - HAProxy: haproxy.org/timeout-http-request: "3600s"
  # - AWS ALB: alb.ingress.kubernetes.io/target-group-attributes: idle_timeout.timeout_seconds=3600
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"    # Allow long-running SSE connections
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"    # Allow long-running SSE connections
    nginx.ingress.kubernetes.io/proxy-buffering: "off"        # Disable buffering for real-time streaming
    nginx.ingress.kubernetes.io/proxy-request-buffering: "off" # Disable request buffering for real-time streaming
  tls:
    enabled: false                  # Enable TLS/HTTPS
    secretName: ""                  # TLS secret name (generated if empty when enabled)
    clusterIssuer: ""               # cert-manager ClusterIssuer name (e.g., "letsencrypt")

# Gateway API configuration (mutually exclusive with Ingress)
# Gateway API is the next-generation successor to Ingress for routing HTTP traffic
# Best practice: Reference an existing platform-managed Gateway (gateway.name)
# Alternative: Create a Gateway resource (gateway.create: true) for dev/testing
gateway:
  # Reference mode (RECOMMENDED): Reference an existing Gateway created by platform team
  name: ""                          # Name of existing Gateway to reference (leave empty to disable Gateway API)
  namespace: ""                     # Namespace of the Gateway (leave empty if same namespace as this release)
  
  # Creation mode (ALTERNATIVE): Create a Gateway resource managed by this chart
  create: false                     # Set to true to create a Gateway resource (for dev/testing scenarios)
  className: ""                     # GatewayClassName (required when create: true, e.g., "istio", "kong", "envoy-gateway")
  
  # Gateway listeners configuration (only used when create: true)
  listeners:
    http:
      enabled: true                 # Enable HTTP listener on port 80
      port: 80                      # HTTP port
      protocol: HTTP                # Protocol type
      hostname: ""                  # Optional: hostname for this listener (e.g., "dot-ai.example.com")
    https:
      enabled: false                # Enable HTTPS listener on port 443
      port: 443                     # HTTPS port
      protocol: HTTPS               # Protocol type
      hostname: ""                  # Optional: hostname for this listener (e.g., "dot-ai.example.com")
      # TLS configuration (only used when https.enabled: true)
      tls:
        mode: Terminate             # TLS mode: Terminate (decrypt at Gateway)
        certificateRefs: []         # List of certificate references (e.g., from cert-manager)
          # Example with cert-manager:
          # - name: dot-ai-tls
          #   kind: Secret
  
  # Gateway annotations (for external-dns, monitoring, etc.)
  annotations: {}
    # Example: external-dns integration
    # external-dns.alpha.kubernetes.io/hostname: "dot-ai.example.com"
  
  # HTTPRoute configuration (applies to both reference and creation modes)
  route:
    # Timeout configuration for SSE (Server-Sent Events) streaming
    # Gateway API controllers may use different timeout annotation patterns
    # Common patterns: Kong uses konghq.com/request-timeout, Istio uses networking.istio.io/timeout
    timeout: 3600s                  # Request timeout (default: 3600s for SSE streaming)
    annotations: {}                 # HTTPRoute-specific annotations
      # Example: Kong Gateway timeout
      # konghq.com/request-timeout: "3600000"  # milliseconds
      # Example: Istio timeout
      # networking.istio.io/timeout: "3600s"

# AI Provider configuration
ai:
  provider: anthropic               # AI provider type (anthropic, anthropic_opus, anthropic_haiku, openai, google, kimi, kimi_thinking, xai, amazon_bedrock)
  model: ""                         # Optional: model override (e.g., "llama3.1:70b", "gpt-4o")

  # Custom endpoint configuration for self-hosted or alternative SaaS providers (PRD #194)
  customEndpoint:
    enabled: false                  # Enable custom endpoint
    baseURL: ""                     # Custom LLM endpoint URL - MUST include /v1 suffix for OpenAI-compatible APIs (e.g., "http://ollama-service:11434/v1")
    embeddingsBaseURL: ""           # Optional: custom embeddings endpoint URL (if different from LLM endpoint, also requires /v1 suffix)
    embeddingsModel: ""             # Optional: custom embeddings model name (e.g., "nomic-embed-text" for Ollama, defaults to "text-embedding-3-small" for OpenAI)
    embeddingsDimensions: ""        # Optional: custom embeddings dimensions (e.g., "768" for nomic-embed-text, defaults to "1536" for OpenAI)

  # Examples (commented out):
  # Example 1: Ollama (self-hosted) - IMPORTANT: Include /v1 suffix
  # ai:
  #   provider: openai
  #   model: "llama3.1:70b"
  #   customEndpoint:
  #     enabled: true
  #     baseURL: "http://ollama-service:11434/v1"  # /v1 suffix is REQUIRED
  #
  # Example 2: Azure OpenAI (SaaS)
  # ai:
  #   provider: openai
  #   model: "gpt-4o"
  #   customEndpoint:
  #     enabled: true
  #     baseURL: "https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT"
  #
  # Example 3: vLLM (self-hosted) - IMPORTANT: Include /v1 suffix
  # ai:
  #   provider: openai
  #   model: "meta-llama/Llama-3.1-70B-Instruct"
  #   customEndpoint:
  #     enabled: true
  #     baseURL: "http://vllm-service:8000/v1"  # /v1 suffix is REQUIRED
  #
  # Note: OpenAI-compatible endpoints (Ollama, vLLM, LocalAI) REQUIRE the /v1 suffix.
  # Without it, API calls will fail with 404 Not Found errors.
  # Custom endpoints must support OpenAI-compatible API and models must
  # support 8K+ output tokens for reliable YAML generation.

# Additional environment variables (optional)
# Use this to add any custom environment variables to the MCP server
# Example use cases: tracing configuration, custom integrations, feature flags
extraEnv: []
  # - name: OTEL_TRACING_ENABLED
  #   value: "true"
  # - name: OTEL_EXPORTER_OTLP_ENDPOINT
  #   value: "http://jaeger-collector:4318/v1/traces"
  # - name: OTEL_SERVICE_NAME
  #   value: "dot-ai-mcp-production"

# dot-ai Controller Solution CR
# Note: Controller must be installed separately before enabling this
# Install: helm install dot-ai-controller oci://ghcr.io/vfarcic/dot-ai-controller/charts/dot-ai-controller:0.14.0 -n dot-ai
controller:
  enabled: false                    # Create Solution CR to track this deployment (requires controller installed separately)

# Qdrant Vector Database
qdrant:
  enabled: true                     # Deploy Qdrant as dependency (false = use external)
  image:
    repository: qdrant/qdrant       # Qdrant image repository
    tag: v1.15.5                    # Qdrant image tag
  external:
    url: ""                         # External Qdrant URL (required when enabled=false)
    apiKey: ""                      # External Qdrant API key (optional)